{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6czvz5VKO5M"
   },
   "source": [
    "# Notebook for Neural Machine Translation\n",
    "Welcome to the programming portion of the assignment! Each assignment throughout the semester will have a written portion and a programming portion. We will be using [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5o8HI5JqTvU5"
   },
   "source": [
    "## Learning Objectives\n",
    "In this problem, we will use [PyTorch](https://pytorch.org/) to implement a sequence-to-sequence (seq2seq) transformer model to build a nerual machine translation (NMT) system, which translates from French to English."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObrHyvWvTyGZ"
   },
   "source": [
    "## Writing Code\n",
    "Look for the keyword \"TODO\" and fill in your code in the empty space.\n",
    "You can edit code in the other parts of the notebook too, which can be useful for debugging, but be careful to avoid breaking the provided code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnYMKJlKNXYe"
   },
   "source": [
    "## Installing Packages\n",
    "\n",
    "Install PyTorch using pip. See [https://pytorch.org/](https://pytorch.org/) if you want to install it on your computer.\n",
    "In addition, we will also be needing [huggingface](https://huggingface.co/)'s `transformers` and `datasets` libraries, and [nltk](https://www.nltk.org/) to compute the BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dRVuiP_JVdT",
    "outputId": "a97dabdf-91f2-4273-d305-d4a6f45d862c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.8.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from torch==1.8.0) (4.12.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from torch==1.8.0) (1.26.4)\n",
      "Requirement already satisfied: transformers==4.27.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (4.27.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from transformers==4.27.0) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from transformers==4.27.0) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from transformers==4.27.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from transformers==4.27.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from transformers==4.27.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from transformers==4.27.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from transformers==4.27.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from transformers==4.27.0) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from transformers==4.27.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.0) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.27.0) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers==4.27.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from requests->transformers==4.27.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from requests->transformers==4.27.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from requests->transformers==4.27.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from requests->transformers==4.27.0) (2025.1.31)\n",
      "Collecting datasets==2.10.0\n",
      "  Using cached datasets-2.10.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets==2.10.0) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets==2.10.0) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets==2.10.0) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets==2.10.0) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets==2.10.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets==2.10.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets==2.10.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets==2.10.0) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from fsspec[http]>=2021.11.1->datasets==2.10.0) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets==2.10.0) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets==2.10.0) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets==2.10.0) (24.2)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets==2.10.0) (0.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets==2.10.0) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from aiohttp->datasets==2.10.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from aiohttp->datasets==2.10.0) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from aiohttp->datasets==2.10.0) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from aiohttp->datasets==2.10.0) (25.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from aiohttp->datasets==2.10.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from aiohttp->datasets==2.10.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from aiohttp->datasets==2.10.0) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from aiohttp->datasets==2.10.0) (1.18.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets==2.10.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets==2.10.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets==2.10.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from requests>=2.19.0->datasets==2.10.0) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from tqdm>=4.62.1->datasets==2.10.0) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from pandas->datasets==2.10.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from pandas->datasets==2.10.0) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from pandas->datasets==2.10.0) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.10.0) (1.17.0)\n",
      "Using cached datasets-2.10.0-py3-none-any.whl (469 kB)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 3.3.2\n",
      "    Uninstalling datasets-3.3.2:\n",
      "      Successfully uninstalled datasets-3.3.2\n",
      "Successfully installed datasets-2.10.0\n"
     ]
    }
   ],
   "source": [
    "# Pytorch is typically already installed in Google Colab (uncomment to install):\n",
    "!pip install torch==1.8.0\n",
    "# or for GPU support:\n",
    "#!pip install torch==1.8.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install transformers==4.27.0\n",
    "!pip install datasets==2.10.0\n",
    "# NLTK is typically also already installed in Google Colab (uncomment to install):\n",
    "# !pip install nltk==3.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ylk_JS66QjsE",
    "outputId": "6644f285-773e-4658-fe7e-79068a2fab99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (2.10.0)\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from aiohttp->datasets) (25.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rob\\qsync\\rug\\year 2\\nlp\\omw\\natural-language-processing\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-3.3.2-py3-none-any.whl (485 kB)\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.10.0\n",
      "    Uninstalling datasets-2.10.0:\n",
      "      Successfully uninstalled datasets-2.10.0\n",
      "Successfully installed datasets-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iw76yPDhOia1"
   },
   "source": [
    "## Download NMT data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AL42TT6Tb0S7"
   },
   "source": [
    "We first download the data for NMT, which contains pairs of parallel sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TmD7lPOXOlm2",
    "outputId": "3a0491b1-127a-4890-9fbf-bd82a85d9378"
   },
   "outputs": [],
   "source": [
    "#!wget -O resources.zip \"https://github.com/Tsegaye-misikir/NLP-rug/raw/main/MLT/resources.zip\"\n",
    "#!unzip -q resources.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5TS8PG0Os0f"
   },
   "source": [
    "## Data preprocessing\n",
    "In this section we will write code to load and tokenize the data for NMT.\n",
    "\n",
    "\n",
    "The parallel data is provided as huggingface datasets, one for each split of `train`, `validation` and `test`. We load it via the `load_from_disk` method and inspect its features. If you'd like to know more about these dataset objects, have a look at [this tutorial](https://huggingface.co/docs/datasets/access)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxi3D2oX1iIK",
    "outputId": "9f4fa594-62ac-4260-ce13-3ed8e262bde5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rob\\Qsync\\Rug\\Year 2\\nlp\\OMW\\Natural-Language-Processing\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of splits: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text_en', 'text_fr'],\n",
      "        num_rows: 8701\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text_en', 'text_fr'],\n",
      "        num_rows: 485\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text_en', 'text_fr'],\n",
      "        num_rows: 486\n",
      "    })\n",
      "})\n",
      "First training example: {'text_en': 'i m tough .', 'text_fr': 'je suis dure .'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "raw_text_datasets = load_from_disk(\"resources/parallel_en_fr_corpus\")\n",
    "print(\"Summary of splits:\", raw_text_datasets)\n",
    "print(\"First training example:\", raw_text_datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1NmS9j01iIK"
   },
   "source": [
    "You are also provided with two pre-trained tokenizers for the source and target languages respectively, which we can load with the hugginface transfomers library. [This tutorial](https://huggingface.co/docs/transformers/preprocessing#natural-language-processing) provides an introduction to using pre-trained tokenizers and the powerful `AutoTokenizer` class. The tokenizers are based on byte-pair encodings which break words into smaller units. This is aimed at reducing the sparsity of words, as subwords can be shared between different rare words. If you are interested in learning more, see the paper [Neural Machine Translation of Rare Words with Subword Units](https://www.aclweb.org/anthology/P16-1162.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iqnyk7Xo1iIK",
    "outputId": "b4a8b71b-8c5c-4e03-f106-60bcce72053c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Example ***\n",
      "Example sentence: we have an example\n",
      "Tokenizer output: {'input_ids': [1, 64, 324, 103, 266, 1490, 92, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Tokens: ['<s>', '▁we', '▁have', '▁an', '▁ex', 'amp', 'le', '</s>']\n",
      "Reconstructed sentence <s> we have an example</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "source_tokenizer = AutoTokenizer.from_pretrained(\"resources/tokenizer_fr\")\n",
    "target_tokenizer = AutoTokenizer.from_pretrained(\"resources/tokenizer_en\")\n",
    "\n",
    "\n",
    "# As a demonstration, we will the following English sentence to tokens.\n",
    "example_sentence = \"we have an example\"\n",
    "tokenizer_output = target_tokenizer(example_sentence)\n",
    "print(\"\\n*** Example ***\")\n",
    "print(\"Example sentence:\", example_sentence)\n",
    "print(\"Tokenizer output:\", tokenizer_output)\n",
    "\n",
    "# We convert every token id to its associated string, but find the special character ▁ which indicates the beginning of a word.\n",
    "# Note that very common words are represented by a single token, while others are split into subunits due to the small vocab size.\n",
    "# Also note that †he tokenizer already adds special tokens to the beginning and end of the sentence.\n",
    "decoded_sequence = [target_tokenizer.decode(token) for token in tokenizer_output[\"input_ids\"]]\n",
    "print(\"Tokens:\", decoded_sequence)\n",
    "\n",
    "# By replacing the special character ▁ with whitespace, we can reconstruct a legibile sentence,\n",
    "# which differs from the original example by special tokens, includings <unk> tokens, and minor whitespace differences.\n",
    "reconstructed = \"\".join(decoded_sequence).replace(\"▁\", \" \")\n",
    "print(\"Reconstructed sentence\", reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ui6m-oh1iIL"
   },
   "source": [
    "We now want to convert the entire dataset to token ids.\n",
    "Specifically, we want to use the tokenizers to create a dataset with\n",
    "features \"encoder_input_ids\" and \"decoder_input_ids\", which both have type `List[int]`\n",
    "and which will later be the inputs to our encoder-decoder model. We will implement this using the powerful `map` function. You can find its API reference [here](https://huggingface.co/docs/datasets/v2.10.0/en/package_reference/main_classes#datasets.Dataset.map)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XPb6HpAH1iIL"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "def map_example(example: Dict[str, str]) -> Dict[str, List[int]]:\n",
    "    # TODO: Tokenize the source and target text for an entry in the parallel dataset\n",
    "    # and return a dictionary with the keys \"encoder_input_ids\" and \"decoder_input_ids\".\n",
    "    # You can use `source_tokenizer` and `target_tokenizer`\n",
    "    french_tokens = source_tokenizer(example['text_fr'])\n",
    "    english_tokens = target_tokenizer(example['text_en'])\n",
    "    return {\"encoder_input_ids\": french_tokens[\"input_ids\"], \"decoder_input_ids\": english_tokens[\"input_ids\"]}\n",
    "    raise NotImplementedError(\"The map_item function is not implemented yet\")\n",
    "\n",
    "# When mapped is applied to the DatasetDict object, it will apply `map` separately to each split.\n",
    "tokenized_datasets = raw_text_datasets.map(map_example, batched=False)\n",
    "\n",
    "# The `remove_columns` removes the existing text features from the new dataset, as they are no longer needed.\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(raw_text_datasets.column_names[\"train\"])\n",
    "\n",
    "# Sanity checks on the new dataset\n",
    "assert set(tokenized_datasets.column_names[\"train\"]) == {\"decoder_input_ids\", \"encoder_input_ids\"}\n",
    "assert len(tokenized_datasets[\"train\"]) == len(raw_text_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3qPsuS71iIL"
   },
   "source": [
    "## Transformer model for NMT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R90j2WL61iIM"
   },
   "source": [
    "We will now implement a encoder-decoder transformer model.\n",
    "We already provide code for the Feedforward Layers and Transformer Blocks, but you will have to implement the MultiHeadAttention and Embedding layer from scratch, as well as registering all the layers in the final EncoderDecoderModel. Pay attention to doc-strings and typing information to understand the context and purpose of each missing code block!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nVjxNtLz1iIM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "from typing import Optional, Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7pMQHadt1iIM"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 num_attention_heads: int,\n",
    "                 is_causal_attention: bool = False,\n",
    "                 is_cross_attention: bool = False):\n",
    "        \"\"\"Defines a flexible multi-head attention layer.\n",
    "\n",
    "        This layer should define parameters for the query, key and value projections, as well as the output projection,\n",
    "        and implement the following steps:\n",
    "        (1) Project the input vectors using query projection and key projection matrices.\n",
    "        (2) Compute the head-wise attention scores scaled by 1/sqrt(head_dim)\n",
    "        (3) Perform appropriate masking to the attention scores using key_padding_mask and optionally causal attention.\n",
    "        (4) Normalize the head-wise attention scores using softmax.\n",
    "        (5) Compute the value projections and then aggregate using the normalized attention scores.\n",
    "        (6) Use the output projection to obtain the final output vectors.\n",
    "        When is_cross_attention is True, the key and value projections are computed from the encoder outputs.\n",
    "        Note that we do not use attention weight dropout in this implementation.\n",
    "\n",
    "        Args:\n",
    "            hidden_size: The dimensionality of the input vectors.\n",
    "            num_attention_heads: The number of attention heads.\n",
    "            is_causal_attention: Whether to use causal masking,\n",
    "                    where tokens cannot attend to the future tokens on their right.\n",
    "            is_cross_attention: Whether to use cross attention,\n",
    "                    where we use different inputs for the key/value vs. query vectors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert hidden_size % num_attention_heads == 0, \"The hidden size must be divisible by the number of attention heads.\"\n",
    "        self.head_dim = hidden_size // num_attention_heads  # embedding dimension of query and key vectors per head\n",
    "\n",
    "        # TODO Initialize the module and its parameters here.\n",
    "        # This module should be able to handle both full self-attention, causal masked self-attention and cross-attention.\n",
    "        # IMPORTANT: You are not allowed to use `nn.MultiheadAttention` or `nn.functional.scaled_dot_product_attention`!\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.head_dim = hidden_size // num_attention_heads  # Size per head\n",
    "\n",
    "        # Projection matrices for Q, K, V\n",
    "        self.query_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.key_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.value_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "        # Output projection (to transform attention output back)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "        # Save flags\n",
    "        self.is_causal_attention = is_causal_attention\n",
    "        self.is_cross_attention = is_cross_attention\n",
    "        \n",
    "\n",
    "        #raise NotImplementedError(\"The __init__ function in TransformerAttention is not implemented yet.\")\n",
    "\n",
    "    def causal_attention_mask(self,\n",
    "                              sequence_length: int,\n",
    "                              device: Optional[torch.device] = None) -> torch.FloatTensor:\n",
    "        \"\"\"Return a Float tensor that can be added to the (un-normalized) attention scores for causal masking.\n",
    "\n",
    "        Args:\n",
    "            sequence_length: width and height of the attention mask tensor.\n",
    "            device: which torch device the resulting tensor should be on (important if you use GPU).\n",
    "\n",
    "        Returns:\n",
    "            A Float tensor of shape (1, 1, sequence_length, sequence_length) on device `device`,\n",
    "            where the entries above the diagonal contain large negative values,\n",
    "            which means that a query at position i can't attend to a key at position j>i.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO Implement the forward function.\n",
    "        # IMPORTANT: For full credit, you should not use python loops.\n",
    "        #\n",
    "        # Hint 1: You can pick an arbitrary large value (e.g., -10^{6}), but note that\n",
    "        #         using `float(\"-inf\")` might lead to numerical issues and 'nan' values during training.\n",
    "        #\n",
    "        # Hint 2: Useful pytorch functions for this are `torch.arange` or `torch.triu`.\n",
    "        #\n",
    "        # Hint 3: You can move the tensor you create to a device by calling `tensor.to(device)`\n",
    "        #\n",
    "        # You should use this function in `forward` and use the returned tensor to implement causal masking\n",
    "        # by adding it to the un-normalized attention scores of shape (batch_size, num_heads, sequence_length, sequence_length),\n",
    "        # as torch will handle broadcasting and expand the first two dimensions to batch size and num_heads.\n",
    "        #\n",
    "        # You will the masking tensor to be on the same device as the attention scores's device,\n",
    "        # which you can via the attribute `tensor.device`.\n",
    "        mask = torch.triu(torch.ones(sequence_length, sequence_length, device=device) * -1e6, diagonal=1)\n",
    "        return mask.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "        raise NotImplementedError(\"The forward function in TransformerAttention is not implemented yet.\")\n",
    "\n",
    "\n",
    "    def forward(self,\n",
    "                hidden_states: torch.FloatTensor,\n",
    "                key_padding_mask: torch.BoolTensor,\n",
    "                encoder_outputs: Optional[torch.FloatTensor] = None) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n",
    "        \"\"\"Computes scaled dot-product attention and returns the output of the attention layer.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Tensor of shape (batch_size, sequence_length, hidden_size) - the input vectors to the layer.\n",
    "            key_padding_mask: Tensor of shape (batch_size, sequence_length) indicating which tokens are padding tokens.\n",
    "                    A `True` entry means that this token should be ignored for the purpose of attention.\n",
    "                    In the case of cross-attention, the tensor has shape (batch_size, encoder_sequence_length).\n",
    "            encoder_outputs: Optional tensor of shape (batch_size, encoder_sequence_length, hidden_size).\n",
    "                    The output vectors of the encoder and only passed if the layer performs cross-attention.\n",
    "\n",
    "        Returns:\n",
    "            A (layer_output, attention_weights) where layer_output is a tensor of shape (batch_size, sequence_length, hidden_size)\n",
    "            and attention_weights are the normalized attention scores in the form of\n",
    "            a tensor of shape (batch_size, num_attention_heads, number_of_query_tokens, number_of_key_tokens).\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO Implement the forward function.\n",
    "        # IMPORTANT: For full credit, you should not use python loops. Furthermore,\n",
    "        #            you are not allowed to use `nn.MultiheadAttention` or `nn.functional.scaled_dot_product_attention`!\n",
    "        #\n",
    "        # Hint 1: Use `torch.reshape` to add a new axis for the attention head,\n",
    "        #         which will allow you to process all attention heads in parallel.\n",
    "        #\n",
    "        # Hint 2: You can use `torch.transpose` to swap the order of two axes,\n",
    "        #         As the attention head dimension should be next to the batch size,\n",
    "        #         see the shape of the output attention weights.\n",
    "        #\n",
    "        # Hint 3: `torch.bmm(matrix1, matrix2)` is useful for computing batched matrix multiplications\n",
    "        #         If matrix1 has shape (B, M, N) and matrix2 has shape (B, N, P),\n",
    "        #         it performs `B` matrix multiplications and outputs a tensor of shape (B, M, P).\n",
    "        #         Alternatively, `torch.einsum` should be very useful.\n",
    "        #         (We really encourage you to check out the documentation of `torch.einsum`,\n",
    "        #         it can really make your life easier here.)\n",
    "        if self.is_cross_attention:\n",
    "            query = self.query_proj(hidden_states)\n",
    "            key = self.key_proj(encoder_outputs)\n",
    "            value = self.value_proj(encoder_outputs)\n",
    "        else:\n",
    "            query = self.query_proj(hidden_states)\n",
    "            key = self.key_proj(hidden_states)\n",
    "            value = self.value_proj(hidden_states)\n",
    "\n",
    "        batch_size, seq_length, _ = hidden_states.size()\n",
    "        query = query.view(batch_size, seq_length, self.num_attention_heads, self.head_dim).transpose(1, 2)\n",
    "        key = key.view(batch_size, -1, self.num_attention_heads, self.head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, self.num_attention_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        dot_product = torch.matmul(query, key.transpose(-2, -1)) \n",
    "        attention_scores = self.scale(dot_product, self.head_dim)\n",
    "\n",
    "        if self.is_causal_attention:\n",
    "            causal_mask = self.causal_attention_mask(seq_length, device=hidden_states.device)\n",
    "            attention_scores = attention_scores + causal_mask\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2), -1e6)\n",
    "\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_output = torch.matmul(attention_weights, value)\n",
    "\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_length, self.hidden_size)\n",
    "        layer_output = self.out_proj(attention_output)\n",
    "\n",
    "        return layer_output, attention_weights\n",
    "\n",
    "        \n",
    "\n",
    "         \n",
    "        \n",
    "        #raise NotImplementedError(\"The forward function in TransformerAttention is not implemented yet.\")\n",
    "    \n",
    "    def scale(self, dot_product: torch.FloatTensor, head_dim: int) -> torch.FloatTensor:\n",
    "        \"\"\"Scales the dot product by the square root of the head dimension.\n",
    "\n",
    "        Args:\n",
    "            dot_product: The dot-product tensor of shape (batch_size, num_heads, sequence_length, sequence_length).\n",
    "            head_dim: The dimensionality of the query and key vectors per head.\n",
    "\n",
    "        Returns:\n",
    "            The dot product scaled by 1/sqrt(head_dim).\n",
    "        \"\"\"\n",
    "        return dot_product / math.sqrt(head_dim)\n",
    "\n",
    "\n",
    "    def softmax(self, matrix):\n",
    "        return F.softmax(matrix, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNhiLcSw1iIM"
   },
   "source": [
    "Before we move on to the other modules, you should implement a sanity check for your attention implementation:\n",
    "1. We check the dimensions of the output of the layer and\n",
    "2. We plot the attention weights to some toy embedding inputs.\n",
    "We assume that the last token in the encoder and the last two tokens in the decoder are pad tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "epu_wgGi1iIM"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embed_dim = 8\n",
    "num_heads = 2\n",
    "batch_size = 1\n",
    "encoder_seq_length = 5\n",
    "decoder_seq_length = 7\n",
    "\n",
    "encoder_outputs = torch.randn(batch_size, encoder_seq_length, embed_dim)\n",
    "decoder_inputs = torch.randn(batch_size, decoder_seq_length, embed_dim)\n",
    "\n",
    "encoder_padding_mask = torch.zeros(batch_size, encoder_seq_length, dtype=torch.bool)\n",
    "encoder_padding_mask[:, -1] = True # The last encoder token is a padding tokens\n",
    "\n",
    "decoder_padding_mask = torch.zeros(batch_size, decoder_seq_length, dtype=torch.bool)\n",
    "decoder_padding_mask[:, -2:] = True # The last two decoder tokens are padding token\n",
    "\n",
    "\n",
    "cross_attention = MultiHeadAttention(embed_dim, num_heads, is_cross_attention=True)\n",
    "causal_attention = MultiHeadAttention(embed_dim, num_heads, is_causal_attention=True)\n",
    "\n",
    "cross_attention_out, cross_attention_weights = cross_attention(decoder_inputs, encoder_padding_mask, encoder_outputs)\n",
    "causal_attention_out, causal_attention_weights = causal_attention(decoder_inputs, decoder_padding_mask)\n",
    "\n",
    "# Make sure your outputs have the right hapes\n",
    "assert cross_attention_out.shape == (batch_size, decoder_seq_length, embed_dim)\n",
    "assert cross_attention_weights.shape == (batch_size, num_heads, decoder_seq_length, encoder_seq_length)\n",
    "assert causal_attention_out.shape == (batch_size, decoder_seq_length, embed_dim)\n",
    "assert causal_attention_weights.shape == (batch_size, num_heads, decoder_seq_length, decoder_seq_length)\n",
    "\n",
    "# Check that the attention weights are normalized\n",
    "assert torch.isclose(cross_attention_weights.sum(dim=-1), torch.tensor(1.0)).all()\n",
    "assert torch.isclose(causal_attention_weights.sum(dim=-1), torch.tensor(1.0)).all()\n",
    "\n",
    "# Check if the attention masking works\n",
    "assert torch.isclose(cross_attention_weights[:,:,:,-1], torch.tensor(0.0)).all()\n",
    "assert torch.isclose(causal_attention_weights[:,:,:,-2:], torch.tensor(0.0)).all()\n",
    "assert torch.isclose(causal_attention_weights[:,:,2,3:], torch.tensor(0.0)).all()\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_attention_matrix(attention_matrix, title):\n",
    "    \"\"\"Creates a new figure and plots the normalized attention weights as a heatmap.\n",
    "\n",
    "    This should provide a colorbar for the scale of the heatmap and label the axes \"query token position\" and \"key token position\".\n",
    "    Args:\n",
    "        attention_matrix: A numpy array of shape (number_of_query_tokens, number_of_key_tokens)\n",
    "        title: The title of the plot.\n",
    "    \"\"\"\n",
    "    # TODO Implement this function\n",
    "    plt.imshow(attention_matrix, cmap='hot', interpolation='nearest')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    #raise NotImplementedError(\"Plotting the attention weights is not implemented yet.\")\n",
    "\n",
    "plot_attention_matrix(cross_attention_weights[0,0].detach().numpy(), \"cross-attention, head 1\")\n",
    "plot_attention_matrix(cross_attention_weights[0,1].detach().numpy(), \"cross-attention, head 2\")\n",
    "plot_attention_matrix(causal_attention_weights[0,0].detach().numpy(), \"causal self-attention, head 1\")\n",
    "plot_attention_matrix(causal_attention_weights[0,1].detach().numpy(), \"causal self-attention, head 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d86pBrD71iIN"
   },
   "outputs": [],
   "source": [
    "class TransformerEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, max_sequence_length: int):\n",
    "        \"\"\"Defines the embedding layer with learnt positional embeddings.\n",
    "\n",
    "        This layer defines both the token embeddings and positional embeddings,\n",
    "        which are added together to form the final embedding.\n",
    "\n",
    "        Args:\n",
    "            vocab_size: The size of the vocabulary,\n",
    "                    used to define the size of the token embedding table.\n",
    "            hidden_size: The dimensionality of the embedding space for both token embeddings and positional embeddings.\n",
    "            max_sequence_length: The maximum sequence length of the input sequences,\n",
    "                    used to define the size of the position embedding table.\n",
    "\n",
    "        Note that this implementation does not use dropout on the embeddings\n",
    "        and uses learnt positional embeddings instead of sinusoidal embeddings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO Initialize the module and its parameters here.\n",
    "        # You should use `nn.Embedding` for both token embeddings and positional embeddings\n",
    "\n",
    "        raise NotImplementedError(\"The __init__ function in TransformerEmbeddings is not implemented yet.\")\n",
    "\n",
    "    def compute_logits(self, decoder_output: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Computes the logits for the next token prediction given the decoder output.\n",
    "\n",
    "        Args:\n",
    "            decoder_output: Tensor of shape (batch_size, sequence_length, hidden_size) - the output of the decoder.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, sequence_length, vocab_size) containing the logits for the next token prediction.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO Implement this function\n",
    "        # Hint: you can access the weight parameter matrix via .weight of an nn.Embedding module:\n",
    "        # Example:\n",
    "        # ```embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        # assert list(embeddings.weight.shape) == [num_embeddings, embedding_dim]```\n",
    "        # torch.matmul or F.linear may also be useful here.\n",
    "\n",
    "        raise NotImplementedError(\"The compute_logits function in TransformerEmbeddings is not implemented yet.\")\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Computes the embeddings for the input tokens.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Tensor of shape (batch_size, sequence_length) containing the input token ids.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, sequence_length, hidden_size) containing\n",
    "                    the sum of token embeddings and position embeddings for the input tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO Implement the forward pass of the embedding layer.\n",
    "        # IMPORTANT: For full credit, you should not use python loops!\n",
    "\n",
    "        raise NotImplementedError(\"The forward function in TransformerEmbeddings is not implemented yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CFW-eNNB1iIN"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 intermediate_size: int,\n",
    "                 num_attention_heads: int,\n",
    "                 hidden_dropout_prob: float,\n",
    "                 is_decoder: bool = False):\n",
    "        \"\"\"Defines a single Transformer block, which can be either for the encoder or the decoder.\n",
    "\n",
    "        Args:\n",
    "            hidden_size: The dimensionality of the input and output vectors of this layer.\n",
    "            intermediate_size: The intermediate size of the feedforward layers.\n",
    "            num_attention_heads: The number of attention heads.\n",
    "            hidden_dropout_prob: The dropout probability for the hidden states.\n",
    "            is_decoder: Whether this block is part of the decoder.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_decoder = is_decoder\n",
    "        self.self_attention = MultiHeadAttention(hidden_size, num_attention_heads, is_causal_attention=is_decoder)\n",
    "        self.self_attention_layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        if is_decoder:\n",
    "            self.cross_attention = MultiHeadAttention(hidden_size, num_attention_heads, is_cross_attention=True)\n",
    "            self.cross_attention_layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(hidden_size, intermediate_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(intermediate_size, hidden_size),\n",
    "            nn.Dropout(hidden_dropout_prob))\n",
    "        self.feedforward_layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self,\n",
    "                hidden_states: torch.FloatTensor,\n",
    "                padding_mask: torch.BoolTensor,\n",
    "                encoder_outputs: Optional[torch.FloatTensor] = None,\n",
    "                encoder_padding_mask: Optional[torch.BoolTensor] = None) -> torch.FloatTensor:\n",
    "        \"\"\"Defines a single Transformer block, either for the encoder or the decoder.\n",
    "\n",
    "        Args:\n",
    "            hidden_states: Tensor of shape (batch_size, sequence_length, hidden_size) - the outputs from the previous layer.\n",
    "            padding_mask: Tensor of shape (batch_size, sequence_length) indicating which tokens are padding tokens.\n",
    "                    A `True` entry means that this token should be ignored for the purpose of attention.\n",
    "            encoder_outputs: Optional tensor of shape (batch_size, encoder_sequence_length, hidden_size),\n",
    "                    which are the output vectors of the encoder. This argument is only used by decoder blocks.\n",
    "            encoder_padding_mask: Optional tensor of shape (batch_size, encoder_sequence_length) indicating\n",
    "                    which encoder tokens are padding tokens. This argument is only used in decoder blocks.\n",
    "                    A `True` entry means that this token should be ignored for the purpose of attention.\n",
    "\n",
    "        \"\"\"\n",
    "        hidden_states = self.self_attention(hidden_states, padding_mask)[0] + hidden_states\n",
    "        hidden_states = self.self_attention_layer_norm(hidden_states)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            hidden_states = self.cross_attention(hidden_states, encoder_padding_mask, encoder_outputs)[0] + hidden_states\n",
    "            hidden_states = self.cross_attention_layer_norm(hidden_states)\n",
    "\n",
    "        hidden_states = self.feedforward(hidden_states) + hidden_states\n",
    "        hidden_states = self.feedforward_layer_norm(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dC_EQSQ51iIN"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 source_vocab_size: int,\n",
    "                 target_vocab_size: int,\n",
    "                 hidden_size: int,\n",
    "                 intermediate_size: int,\n",
    "                 num_attention_heads: int,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 max_sequence_length: int,\n",
    "                 hidden_dropout_prob: float):\n",
    "        \"\"\"A encoder-decoder transformer model which can be used for NMT.\n",
    "\n",
    "        Args:\n",
    "            source_vocab_size: The size of the source vocabulary.\n",
    "            target_vocab_size: The size of the target vocabulary.\n",
    "            hidden_size: The dimensionality of all input and output embeddings.\n",
    "            intermediate_size: The intermediate size in the feedforward layers.\n",
    "            num_attention_heads: The number of attention heads in each multi-head attention modules.\n",
    "            num_encoder_layers: The number of transformer blocks in the encoder.\n",
    "            num_decoder_layers: The number of transformer blocks in the decoder.\n",
    "            max_sequence_length: The maximum sequence length that this model can handle.\n",
    "            hidden_dropout_prob: The dropout probability in the hidden state in each block.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        # TODO Register the input embedding modules and the encoder and decoder blocks.\n",
    "        # You should use the TransformerBlock and TransformerEmbeddings sub-modules.\n",
    "        #\n",
    "        # Hint: Check out `nn.ModuleList` to register a variable number of sub-modules.\n",
    "\n",
    "        raise NotImplementedError(\"The __init__ function is not implemented yet.\")\n",
    "\n",
    "    def forward_encoder(self, input_ids: torch.LongTensor, padding_mask: torch.BoolTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Implement the forward pass of the encoder.\n",
    "\n",
    "        Args:\n",
    "            input_ids: tensor of shape (batch_size, sequence_length) containing the input token ids to the encoder.\n",
    "            padding_mask: tensor of shape (batch_size, sequence_length) indicating which encoder tokens are padding tokens (`True`)\n",
    "                    and should be ignored in self-attention computations.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, sequence_length, hidden_size) containing the output embeddings of the encoder.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO Implement this function\n",
    "\n",
    "        raise NotImplementedError(\"The forward_encoder function is not implemented yet.\")\n",
    "\n",
    "    def forward_decoder(self,\n",
    "                        input_ids: torch.LongTensor,\n",
    "                        padding_mask: torch.BoolTensor,\n",
    "                        encoder_outputs: torch.FloatTensor,\n",
    "                        encoder_padding_mask: torch.BoolTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Implement the forward pass of the decoder.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Tensor of shape (batch_size, sequence_length) containing the input token ids to the decoder.\n",
    "            padding_mask: Tensor of shape (batch_size, sequence_length) indicating which decoder tokens are padding tokens (`True`)\n",
    "                    and should be ignored in self-attention computations.\n",
    "            encoder_outputs: Tensor of shape (batch_size, encoder_sequence_length, hidden_size) containing the output embeddings of the encoder.\n",
    "            encoder_padding_mask: Tensor of shape (batch_size, encoder_sequence_length) indicating which encoder tokens are padding tokens (`True`)\n",
    "                    and should be ignored in cross-attention computations.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (batch_size, sequence_length, target_vocabulary_size)\n",
    "            containing the logits for predicting the next token in the target sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO Implement this function\n",
    "\n",
    "        raise NotImplementedError(\"The forward_decoder function has to be implemented.\")\n",
    "\n",
    "    def forward(self, encoder_input_ids, encoder_padding_mask, decoder_input_ids, decoder_padding_mask):\n",
    "        encoder_outputs = self.forward_encoder(encoder_input_ids, encoder_padding_mask)\n",
    "        decoder_logits = self.forward_decoder(decoder_input_ids, decoder_padding_mask, encoder_outputs, encoder_padding_mask)\n",
    "        return decoder_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mbf1FkZteHIW"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "In this section, we train the seq2seq model on the parallel tokenized corpus.\n",
    "Before you start training models, you should implement and test the model and its sub-modules, especially the attention.\n",
    "\n",
    "First, we implement a `collate` function, which takes a list of examples from the dataset and forms a batch,\n",
    "consisting of padded encoder and decoder input ids, as well as encoder and decoder padding masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VknfRSbb1iIN"
   },
   "outputs": [],
   "source": [
    "def collate_fn(examples: List[Dict[str, List[int]]]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Collates a list of variable length sequences from the dataset into a batch of pytorch tensors with padding.\"\"\"\n",
    "\n",
    "    encoder_sequence_length = max(len(example[\"encoder_input_ids\"]) for example in examples)\n",
    "    decoder_sequence_length = max(len(example[\"decoder_input_ids\"]) for example in examples)\n",
    "    batch_size = len(examples)\n",
    "\n",
    "    encoder_input_ids = torch.full((batch_size, encoder_sequence_length),\n",
    "                                   fill_value=source_tokenizer.pad_token_id,\n",
    "                                   dtype=torch.int64)\n",
    "    encoder_padding_mask = torch.ones((batch_size, encoder_sequence_length),\n",
    "                                      dtype=torch.bool)\n",
    "\n",
    "    decoder_input_ids = torch.full((batch_size, decoder_sequence_length),\n",
    "                                   fill_value=target_tokenizer.pad_token_id,\n",
    "                                   dtype=torch.int64)\n",
    "    decoder_padding_mask = torch.ones((batch_size, decoder_sequence_length),\n",
    "                                      dtype=torch.bool)\n",
    "\n",
    "    for i, example in enumerate(examples):\n",
    "        encoder_input_ids[i, :len(example[\"encoder_input_ids\"])] = torch.tensor(example[\"encoder_input_ids\"])\n",
    "        encoder_padding_mask[i, :len(example[\"encoder_input_ids\"])] = False\n",
    "\n",
    "        decoder_input_ids[i, :len(example[\"decoder_input_ids\"])] = torch.tensor(example[\"decoder_input_ids\"])\n",
    "        decoder_padding_mask[i, :len(example[\"decoder_input_ids\"])] = False\n",
    "\n",
    "    return {\"encoder_input_ids\": encoder_input_ids,\n",
    "            \"encoder_padding_mask\": encoder_padding_mask,\n",
    "            \"decoder_input_ids\": decoder_input_ids,\n",
    "            \"decoder_padding_mask\": decoder_padding_mask}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mgp3PZL21iIN"
   },
   "source": [
    "Next, we provide a simple training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeZgvg2VkRQd"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def compute_loss_per_token(model, batch):\n",
    "    logits = model(**batch)\n",
    "\n",
    "    valid_label_mask = ~(batch[\"decoder_padding_mask\"][:,1:])\n",
    "    labels = batch[\"decoder_input_ids\"][:,1:][valid_label_mask]\n",
    "    logits = logits[:,:-1][valid_label_mask]\n",
    "\n",
    "    return F.cross_entropy(logits, labels, reduction='none')\n",
    "\n",
    "\n",
    "def evaluate_perplexity(model, dataset, batch_size=32, device=\"cpu\"):\n",
    "    model.eval()\n",
    "\n",
    "    dev_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    num_tokens = loss_sum = 0\n",
    "\n",
    "    # no_grad() signals backend to throw away all gradients\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            # Move tensors in batch to device\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "            token_losses = compute_loss_per_token(model, batch)\n",
    "\n",
    "            loss_sum += token_losses.sum()\n",
    "            num_tokens += token_losses.numel()\n",
    "\n",
    "        dev_ppl = (loss_sum / num_tokens).exp().cpu().item()\n",
    "    return dev_ppl\n",
    "\n",
    "\n",
    "def train(model, training_dataset, validation_dataset,\n",
    "          batch_size=32, lr=1e-3, max_epoch=10, log_every=10, valid_niter=100,\n",
    "          model_path=\"model.pt\"):\n",
    "    model.train()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('Use device: %s' % device)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    epoch = global_step = loss_sum = num_tokens = num_examples = 0\n",
    "    best_valid_perplexity = float('inf')\n",
    "    train_time = begin_time = time.time()\n",
    "    print('Beginning maximum likelihood training')\n",
    "\n",
    "\n",
    "    while True:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            training_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "        epoch += 1\n",
    "        batches_per_epoch = len(train_loader)\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            global_step += 1\n",
    "\n",
    "            # Move tensors in batch to device\n",
    "            for key in batch:\n",
    "                batch[key] = batch[key].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            token_losses = compute_loss_per_token(model, batch)\n",
    "            total_loss = token_losses.sum()\n",
    "\n",
    "            loss = total_loss / batch_size\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_sum += total_loss.cpu().item()\n",
    "            num_tokens += token_losses.numel()\n",
    "            num_examples += batch_size\n",
    "\n",
    "            if global_step % log_every == 0:\n",
    "                average_loss = loss_sum / num_examples\n",
    "                average_ppl = math.exp(loss_sum / num_tokens)\n",
    "                print(f\"epoch {epoch} ({i}/{batches_per_epoch}) | step {global_step} | \"\n",
    "                      f\"avg_nll={average_loss:.2f} avg_ppl={average_ppl:.2f} \"\n",
    "                      f\"speed={num_tokens / (time.time() - train_time):.2f} words/sec \"\n",
    "                      f\"time_elapsed={time.time() - begin_time:.2f} sec\")\n",
    "\n",
    "                train_time = time.time()\n",
    "                loss_sum = num_tokens = num_examples = 0.0\n",
    "\n",
    "            if global_step % valid_niter == 0:\n",
    "                print('Begin validation ...')\n",
    "                dev_perplexity = evaluate_perplexity(model, validation_dataset, batch_size=batch_size, device=device)\n",
    "\n",
    "                print(f\"validation: step {global_step} | dev_ppl={dev_perplexity}\")\n",
    "\n",
    "                if dev_perplexity < best_valid_perplexity:\n",
    "                    best_valid_perplexity = dev_perplexity\n",
    "                    print(f\"epoch {epoch} step {global_step}: save currently the best model to '{model_path}'\")\n",
    "                    torch.save(model.state_dict(), model_path)\n",
    "                    torch.save(optimizer.state_dict(), model_path + '.optim')\n",
    "                model.train()\n",
    "\n",
    "        if epoch == max_epoch:\n",
    "            print('Reached maximum number of epochs')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8NLae1r1iIO"
   },
   "source": [
    "Let's train a relatively small model architecture for 15 epochs.\n",
    "With a reasonable implementation, this should take about 16 minutes on CPU / 3 minutes on GPU and we should achieve a validation perplexity of below 10!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7j8KNRx21iIO"
   },
   "outputs": [],
   "source": [
    "# Set a random seed, so you obtain the same output model if you run this cell again.\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "model = EncoderDecoderModel(\n",
    "    source_vocab_size=source_tokenizer.vocab_size,\n",
    "    target_vocab_size=target_tokenizer.vocab_size,\n",
    "    hidden_size=32,\n",
    "    intermediate_size=32 * 4,\n",
    "    num_attention_heads=4,\n",
    "    num_encoder_layers=3,\n",
    "    num_decoder_layers=3,\n",
    "    max_sequence_length=32,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    )\n",
    "\n",
    "print(\"Model architecture:\", model)\n",
    "print(\"Total number of trainable model parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "train(model, tokenized_datasets[\"train\"], tokenized_datasets[\"validation\"],\n",
    "      max_epoch=15, model_path=\"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xhk9wiHzpyuF"
   },
   "source": [
    "## Evaluate the model\n",
    "\n",
    "We have trained a seq2seq model for the NMT task. Now let's evaluate the model on the test set by generating translations with beam search and comparing them to the gold translations using the BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqEeZlyeuzG2"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def beam_search(model: EncoderDecoderModel,\n",
    "                encoder_input_ids: torch.LongTensor,\n",
    "                beam_width: int = 5,\n",
    "                max_len: int = 32) -> Tuple[torch.LongTensor, float]:\n",
    "    \"\"\"Run beam search on the encoder-decoder model for a single source sequence.\n",
    "\n",
    "    Args:\n",
    "        model: The encoder-decoder model.\n",
    "        encoder_input_ids: The input sequence. Tensor of shape [encoder_sequence_length].\n",
    "        beam_width: Number of generations to expand at each time step.\n",
    "        max_len: Stop generation when reaching this length for the generated sequence.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (generation, score) where generation is the generated target sequence and\n",
    "            a tensor of shape [target_sequence_length] and score is the corresponding\n",
    "            log-probability of this generation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    encoder_input_ids = encoder_input_ids.unsqueeze(0) # Add the batch dimension\n",
    "    encoder_padding_mask = torch.zeros_like(encoder_input_ids, dtype=torch.bool) # No padding\n",
    "    encoder_outputs = model.forward_encoder(encoder_input_ids, encoder_padding_mask)\n",
    "\n",
    "    generations = [torch.tensor([target_tokenizer.bos_token_id], device=encoder_input_ids.device)]\n",
    "    scores = [0.0]\n",
    "\n",
    "    best_generation = None\n",
    "    best_score = float('-inf')\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        new_generations = []\n",
    "        new_scores = []\n",
    "        for score, generation in zip(scores, generations):\n",
    "            generation = generation.unsqueeze(0) # Add the batch dimension\n",
    "            padding_mask = torch.zeros_like(generation, dtype=torch.bool) # No padding\n",
    "            decoder_output = model.forward_decoder(generation, padding_mask, encoder_outputs, encoder_padding_mask)\n",
    "            last_log_probs = decoder_output[0, -1, :].log_softmax(dim=-1)\n",
    "            top_log_probs, top_indices = last_log_probs.topk(beam_width, dim=-1)\n",
    "\n",
    "            new_generations.append(torch.cat([generation.expand(beam_width, -1), top_indices[:,None]], dim=1))\n",
    "            new_scores.append(score + top_log_probs)\n",
    "\n",
    "        new_generations = torch.cat(new_generations, dim=0)\n",
    "        new_scores = torch.cat(new_scores, dim=0)\n",
    "\n",
    "        ends_with_eos = target_tokenizer.eos_token_id == new_generations[:,-1]\n",
    "\n",
    "        if ends_with_eos.any():\n",
    "            new_completed_generations = new_generations[ends_with_eos]\n",
    "            new_completed_scores = new_scores[ends_with_eos]\n",
    "\n",
    "            if new_completed_scores.max() > best_score:\n",
    "                best_score = new_completed_scores.max()\n",
    "                best_generation = new_completed_generations[new_completed_scores.argmax()]\n",
    "\n",
    "        if best_score >= new_scores.max():\n",
    "            break\n",
    "\n",
    "        scores, indices = torch.topk(new_scores, beam_width, dim=-1)\n",
    "        generations = new_generations[indices]\n",
    "\n",
    "    if best_generation is None:\n",
    "        best_generation = generations[0]\n",
    "        best_score = scores[0]\n",
    "\n",
    "    return best_generation, best_score.cpu().item()\n",
    "\n",
    "\n",
    "def run_generation(model, test_dataset, beam_size=5, max_decoding_time_step=32):\n",
    "    \"\"\"Run beam search decoding on the test set, compute BLEU and return reference and candidate target sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('Use device: %s' % device)\n",
    "\n",
    "    input_sentences = []\n",
    "    reference_sentences = []\n",
    "    candidate_sentences = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for example in tqdm(test_dataset):\n",
    "            encoder_input_ids = torch.tensor(example[\"encoder_input_ids\"], device=device)\n",
    "\n",
    "            generation, _ = beam_search(model, encoder_input_ids, beam_size, max_decoding_time_step)\n",
    "\n",
    "            # Decode given source sequence and generated target sequence and avoid special tokens\n",
    "\n",
    "            input_text = \"\".join(source_tokenizer.decode(token).replace(\"▁\", \" \") for token in example[\"encoder_input_ids\"][1:-1])\n",
    "            reference_text = \"\".join(target_tokenizer.decode(token).replace(\"▁\", \" \") for token in example[\"decoder_input_ids\"][1:-1])\n",
    "            candidate_text = \"\".join(target_tokenizer.decode(token).replace(\"▁\", \" \") for token in generation[1:-1].cpu())\n",
    "\n",
    "            reference_sentences.append(reference_text)\n",
    "            candidate_sentences.append(candidate_text)\n",
    "            input_sentences.append(input_text)\n",
    "\n",
    "\n",
    "    bleu_score = corpus_bleu([[ref] for ref in reference_sentences],\n",
    "                             [candidate for candidate in candidate_sentences])\n",
    "\n",
    "    return bleu_score, input_sentences, reference_sentences, candidate_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRjz11ntwgkK"
   },
   "outputs": [],
   "source": [
    "# Restore the best validation checkpoint\n",
    "model.load_state_dict(torch.load(\"model.pt\"))\n",
    "\n",
    "bleu_score, inputs, references, candidates = run_generation(model, tokenized_datasets[\"test\"])\n",
    "print('\\n\\nCorpus BLEU: {}'.format(bleu_score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOLmb9eh6kua"
   },
   "source": [
    "Let's look at some examples. What do you think of the quality of the translations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5e1cDYe6t0B"
   },
   "outputs": [],
   "source": [
    "# Feel free to change the range to look at more samples!\n",
    "for k in range(10, 20):\n",
    "  print(f\"===== Sample {k} =====\")\n",
    "  print(f\"Input: {inputs[k]}\")\n",
    "  print(f\"Gold: {references[k]}\")\n",
    "  print(f\"Pred: {candidates[k]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v648UANH1iIO"
   },
   "source": [
    "# Questions\n",
    "\n",
    "Please answer written questions from the assignment in this section. You can alter the provided code to obtain the answers, but be careful not to break anything!\n",
    "\n",
    "**(b) (4 points)**\n",
    "\n",
    "- (i)\n",
    "    What vocabulary size are we using for the source and target language?\n",
    "\n",
    "- (ii)\n",
    "    Approximately how many source and target tokens are on average contained in a training batch? What proportion of these tokens are `<pad>` tokens on average?\n",
    "\n",
    "- (iii)\n",
    "What is the specific purpose of saving the model parameters in a file `model.pt` throughout training in the code we provide?\n",
    "\n",
    "**(c) (2 points)**\n",
    "Manually look at some results and compare them with the gold answers. What do you think of the quality of the translations? Are these grammatical English sentences? Can you identify any common mistakes?\n",
    "\n",
    "**(d) (4 points)**\n",
    "Consider the provided beam search method. This implementation is not efficient and performs a lot of repeated computation. Identify the issue and propose how you can fix it.\n",
    "In particular, describe how would you have to change the arguments and return values of your EncoderDecoderModel and its sub-modules?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LvPHiteAvaN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Comparison with Pretrained LLMs (10 points)**\n",
    "\n",
    "1.\tObjective: cCompare the performance of your trained Transformer-based NMT model with that of a pretrained large language model (LLM) available through Hugging Face.\n",
    "\n",
    "2.\tProcedure:\n",
    "- Select a Pretrained Model:\n",
    "    Choose a pretrained LLM from Hugging Face (e.g., MarianMT, T5, or any translationcapable model). Briefly describe your choice\n",
    "\n",
    "- Experimental Setup:\n",
    "    Run the selected LLM on the test set. Ensure that the experimental setup (e.g., input formatting, temperature settings, etc.) is documented.\n",
    "\n",
    "-  Quantitative Evaluation:\n",
    "    Compute the BLEU score for the pretrained model’s translations.\n",
    "\n",
    "- Qualitative Analysis:\n",
    "    Manually inspect several translation examples from both your model and the pretrained LLM. Compare aspects such as fluency, grammatical correctness, and adequacy of translation.\n",
    "    \n",
    "- Discussion:\n",
    "    Discuss the differences in performance, computational resource requirements, and any potential trade-offs between your custom-built model and the pretrained Hugging Face model.\n",
    "\n",
    "**Deliverable**\n",
    "\n",
    "- Include your code, evaluation results, and a brief discussion in the notebook or a separate pdf file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The implementation for the pretrained LLM goes here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
