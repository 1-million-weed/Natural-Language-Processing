{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TX7_-K31Zp7y"
   },
   "source": [
    "# Notebook for Programming Question 1\n",
    "Welcome to the programming portion of the assignment! you will be using [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true), so if you have never used it before, take a quick look through this introduction: [Working with Google Colab](https://docs.google.com/document/d/1LlnXoOblXwW3YX-0yG_5seTXJsb3kRdMMRYqs8Qqum4/edit?usp=sharing).\n",
    "\n",
    "you be also be programming in Python, which we will assume a basic familiarity with. Python has fantastic community support and we'll be using numerous packages for machine learning (ML) and natural language processing (NLP) tasks.\n",
    "\n",
    "### Learning Objectives\n",
    "In this problem we will experiment with language models and implement smoothing. We will also see effects of using unigram/bigram LMs and the size of the training data.\n",
    "\n",
    "### Writing Code\n",
    "Look for the keyword \"TODO\" and fill in your code in the empty space.\n",
    "Feel free to add and delete arguments in function signatures, but be careful that you might need to change them in function calls which are already present in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPv_JEf8ZvIb"
   },
   "source": [
    "### Data preprocessing\n",
    "\n",
    "In this section we will write code to load data and clean (tokenize) it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2apYXiYZxog",
    "outputId": "823fcc6e-5447-4a7f-f0b9-77be03e2b111"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries for preprocessing\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "naV4YPZvZ3JO"
   },
   "outputs": [],
   "source": [
    "# Tokenizer class\n",
    "# Fill all function blocks marked as TODO\n",
    "\n",
    "class Tokenizer:\n",
    "  def __init__(self, tokenize_type='basic', lowercase=False):\n",
    "    self.lowercase = lowercase  # If this is True, convert text to lowercase while tokenizing.\n",
    "    self.type = tokenize_type\n",
    "    self.vocab = []\n",
    "\n",
    "\n",
    "  \"\"\"This simple tokenizer splits the text using whitespace.\"\"\"\n",
    "  def basicTokenize(self, string):\n",
    "    words = string.split()\n",
    "    return words\n",
    "\n",
    "  ### TODO : Fill in this function to use NLTK's word_tokenize() function. ###\n",
    "  def nltkTokenize(self, string):\n",
    "    ##### SOLUTION START #####\n",
    "    return nltk.word_tokenize(string)\n",
    "    ##### SOLUTION END #####\n",
    "    \n",
    "  def tokenize(self, string):\n",
    "    if self.lowercase:\n",
    "      string = string.lower()\n",
    "    if self.type == 'basic':\n",
    "      tokens = self.basicTokenize(string)\n",
    "    elif self.type == 'nltk':\n",
    "      tokens = self.nltkTokenize(string)\n",
    "    else:\n",
    "      raise ValueError('Unknown tokenization type.')\n",
    "\n",
    "\n",
    "    # Populate vocabulary\n",
    "    self.vocab += [w for w in set(tokens) if w not in self.vocab]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "  ### TODO: Fill in this function to return the top k words (and their frequency) in the corpus according to frequency. ###\n",
    "  def countTopWords(self, words, k):\n",
    "    ##### SOLUTION START #####\n",
    "    fdist = nltk.FreqDist(word.lower() for word in self.tokenize(words))\n",
    "    fdist.most_common(k)\n",
    "    ##### SOLUTION END #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4qxQp2VJaZhq"
   },
   "outputs": [],
   "source": [
    "# Function for reading the corpus\n",
    "def readCorpus(filename, tokenizer):\n",
    "  with open(filename) as f:\n",
    "    words = tokenizer.tokenize(f.read())\n",
    "  return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXYBlcTyagBQ"
   },
   "source": [
    "### Language Modeling and Smoothing\n",
    "In this section we will first compute the bigram counts and estimate bigram probabilities. We will then implement add-alpha smoothing to modify the probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "K47R17s-ao6k"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "# Feel free to import as many as you like\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BK7uqvcEa49P"
   },
   "outputs": [],
   "source": [
    "# Class definition for language modeling\n",
    "# Fill all function blocks marked as TODO\n",
    "\n",
    "class LanguageModel:\n",
    "  def __init__(self, vocab, n=2, smoothing=None, smoothing_param=None):\n",
    "    assert n >=2, \"This code does not allow you to train unigram models.\"\n",
    "    self.vocab = vocab\n",
    "    self.token_to_idx = {word: i for i, word in enumerate(self.vocab)}\n",
    "    self.n = n\n",
    "    self.smoothing = smoothing\n",
    "    self.smoothing_param = smoothing_param\n",
    "    self.bi_counts = None      # Holds the bigram counts\n",
    "    self.bi_prob = None        # Holds the computed bigram probabilities.\n",
    "\n",
    "    assert smoothing is None or smoothing_param is not None, \"Forgot to specify a smoothing parameter?\"\n",
    "\n",
    "\n",
    "  \"\"\"Compute basic bigram probabilities (without any smoothing)\"\"\"\n",
    "  def computeBigramProb(self):\n",
    "    self.bi_prob = self.bi_counts.copy()\n",
    "\n",
    "    for i, _ in enumerate(tqdm(self.bi_prob, desc=\"Estimating bigram probabilities\")):\n",
    "      cnt = np.sum(self.bi_prob[i])\n",
    "      if cnt > 0:\n",
    "        self.bi_prob[i] /= cnt\n",
    "\n",
    "  ### TODO: complete ###\n",
    "  def computeBigramProbAddAlpha(self, alpha=0.001):\n",
    "    ##### SOLUTION START #####\n",
    "\n",
    "    ##### SOLUTION END #####\n",
    "    return\n",
    "\n",
    "\n",
    "  \"\"\"Train a basic n-gram language model\"\"\"\n",
    "  def train(self, corpus):\n",
    "    if self.n==2:\n",
    "      self.bi_counts = np.zeros((len(self.vocab), len(self.vocab)), dtype=float)\n",
    "    else:\n",
    "      raise ValueError(\"Only bigram model has been implemented so far.\")\n",
    "\n",
    "    # Convert to token indices.\n",
    "    corpus = [self.token_to_idx[w] for w in corpus]\n",
    "\n",
    "    # Gather counts\n",
    "    for i, idx in enumerate(tqdm(corpus[:1-self.n], desc=\"Counting\")):\n",
    "      self.bi_counts[idx][corpus[i+1]] += 1\n",
    "\n",
    "    # Pre-compute the probabilities.\n",
    "    if not self.smoothing:\n",
    "      self.computeBigramProb()\n",
    "    elif self.smoothing == 'addAlpha':\n",
    "      self.computeBigramProbAddAlpha(self.smoothing_param)\n",
    "    else:\n",
    "      raise ValueError(\"Unknown smoothing type.\")\n",
    "\n",
    "\n",
    "\n",
    "  def test(self, corpus):\n",
    "    logprob = 0.\n",
    "\n",
    "    # Convert to token indices.\n",
    "    corpus = [self.token_to_idx[w] for w in corpus]\n",
    "\n",
    "    for i, idx in enumerate(tqdm(corpus[:1-self.n], desc=\"Evaluating\")):\n",
    "      logprob += np.log(self.bi_prob[idx, corpus[i+1]])\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "\n",
    "    logprob /= len(corpus[:1-self.n])\n",
    "\n",
    "    # Compute perplexity\n",
    "    ppl = np.exp(-logprob)\n",
    "\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fkIAhWqYeUgL"
   },
   "source": [
    "### Instantiate a tokenizer and LM, and calculate perplexity\n",
    "This section contains driver code for learning a language model and evaluating it on a train and dev corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "nILv3Tyleb54"
   },
   "outputs": [],
   "source": [
    "def runLanguageModel(train_corpus,\n",
    "                     val_corpus,\n",
    "                     train_fraction,\n",
    "                     tokenizer,\n",
    "                     smoothing_type=None,\n",
    "                     smoothing_param='0.0'):\n",
    "\n",
    "  # Print the top 10 words in the corpus.\n",
    "  # IMPORTANT: complete the function within the tokenizer class in data.py first.\n",
    "  print(\"Top 10 words: %s\" %(tokenizer.countTopWords(train_corpus, k=10)))\n",
    "\n",
    "  # Instantiate the language model.\n",
    "  lm = LanguageModel(tokenizer.vocab, n=2, smoothing=smoothing_type, smoothing_param=smoothing_param)\n",
    "\n",
    "  # Figure out index for a specific percentage of train corpus to use.\n",
    "  train_idx = int(train_fraction * len(train_corpus))\n",
    "\n",
    "  lm.train(train_corpus[:train_idx])\n",
    "\n",
    "  train_ppl = lm.test(train_corpus[:train_idx])\n",
    "  val_ppl = lm.test(val_corpus)\n",
    "\n",
    "  print(\"Train perplexity: %f, Val Perplexity: %f\" %(train_ppl, val_ppl))\n",
    "\n",
    "  return [train_ppl, val_ppl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8hSR_9VczRI"
   },
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "syUH84NUc1rL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'brown-train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m nltk_tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(tokenize_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m'\u001b[39m, lowercase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Read the corpus and store\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m train_corpus \u001b[38;5;241m=\u001b[39m \u001b[43mreadCorpus\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbrown-train.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m val_corpus \u001b[38;5;241m=\u001b[39m readCorpus(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrown-test.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, basic_tokenizer)\n\u001b[0;32m     14\u001b[0m train_corpus_nltk \u001b[38;5;241m=\u001b[39m readCorpus(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrown-train.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, nltk_tokenizer)\n",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m, in \u001b[0;36mreadCorpus\u001b[1;34m(filename, tokenizer)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mreadCorpus\u001b[39m(filename, tokenizer):\n\u001b[1;32m----> 3\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      4\u001b[0m     words \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m      5\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m words\n",
      "File \u001b[1;32mc:\\Users\\Rob\\Qsync\\Rug\\Year 2\\nlp\\OMW\\Natural-Language-Processing\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'brown-train.txt'"
     ]
    }
   ],
   "source": [
    "# Load training data here\n",
    "# First download the datasets\n",
    "\n",
    "!wget https://github.com/Tsegaye-misikir/NLP-rug/blob/main/brown/brown-train.txt\n",
    "!wget https://github.com/Tsegaye-misikir/NLP-rug/blob/main/brown/brown-test.txt\n",
    "\n",
    "# Instantiate a basic tokenizer\n",
    "basic_tokenizer = Tokenizer(tokenize_type='basic', lowercase=True)\n",
    "nltk_tokenizer = Tokenizer(tokenize_type='nltk', lowercase=True)\n",
    "\n",
    "# Read the corpus and store\n",
    "train_corpus = readCorpus('brown-train.txt', basic_tokenizer)\n",
    "val_corpus = readCorpus('brown-test.txt', basic_tokenizer)\n",
    "train_corpus_nltk = readCorpus('brown-train.txt', nltk_tokenizer)\n",
    "val_corpus_nltk = readCorpus('brown-test.txt', nltk_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtrPOKd1vvyx"
   },
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anwiyViHMCTt"
   },
   "source": [
    "#### Plot the frequency of words\n",
    "Code for sub-part (a)(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1xEdVHxMl0n"
   },
   "outputs": [],
   "source": [
    "# TODO: Code for plotting the frequency of words\n",
    "##### SOLUTION START #####\n",
    "\n",
    "##### SOLUTION END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAWeJQ8VdBHU"
   },
   "source": [
    "#### Report the train and test perplexity after learning the language model\n",
    "Code for sub-part (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xH79REFCcNRH"
   },
   "outputs": [],
   "source": [
    "# Train and test perplexity\n",
    "runLanguageModel(train_corpus, val_corpus, train_fraction=1.0, tokenizer=basic_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4ntdVf2j24K"
   },
   "source": [
    "#### Add-alpha smoothing\n",
    "Code for sub-part (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmMeDg-lkQkC"
   },
   "outputs": [],
   "source": [
    "# Example command\n",
    "runLanguageModel(train_corpus, val_corpus, train_fraction=1.0, tokenizer=basic_tokenizer, smoothing_type='addAlpha', smoothing_param=0.01)\n",
    "\n",
    "# TODO: Part (d)\n",
    "##### SOLUTION START #####\n",
    "\n",
    "\n",
    "##### SOLUTION END #####"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
