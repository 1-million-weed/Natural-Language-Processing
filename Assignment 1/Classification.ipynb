{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VSjS_XAbjQI"
      },
      "source": [
        "# Notebook for Programming Question 3\n",
        "Welcome to the programming portion of the assignment! . We will be using [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true), so if you have never used it before, take a quick look through this introduction: [Working with Google Colab](https://docs.google.com/document/d/1LlnXoOblXwW3YX-0yG_5seTXJsb3kRdMMRYqs8Qqum4/edit?usp=sharing).\n",
        "\n",
        "We'll also be programming in Python, which we will assume a basic familiarity with. Python has fantastic community support and we'll be using numerous packages for machine learning (ML) and natural language processing (NLP) tasks.\n",
        "\n",
        "### Learning Objectives\n",
        "In this problem we will implement logistic regression and test it on a sentiment analysis dataset.\n",
        "\n",
        "### Writing Code\n",
        "Look for the keyword \"TODO\" and fill in your code in the empty space.\n",
        "HINT: Adding a bias is equivalent to adding a special token to your features (e.g. \\<BIAS\\>) with count = 1. This could simplify your implementation (although this is not required)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AslCbBsMbol6"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOV13zuwihm6"
      },
      "source": [
        "#### Class and function for loading data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlf4P1apdf-w"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "# Define a class to store a single sentiment example\n",
        "class SentimentExample:\n",
        "    def __init__(self, words, label):\n",
        "        self.words = words\n",
        "        self.label = label # 0 or 1\n",
        "\n",
        "    def __repr__(self):\n",
        "        return repr(self.words) + \"; label=\" + repr(self.label)\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()\n",
        "\n",
        "\n",
        "# Reads sentiment examples in the format [0 or 1]<TAB>[raw sentence]; tokenizes and cleans the sentences.\n",
        "def read_sentiment_examples(infile) -> list[SentimentExample]:\n",
        "    f = open(infile, encoding='iso8859')\n",
        "    exs = []\n",
        "    for line in f:\n",
        "            fields = line.strip().split(\" \")\n",
        "            label = 0 if \"0\" in fields[0] else 1\n",
        "            exs.append(SentimentExample(fields[1:], label))\n",
        "    f.close()\n",
        "    return exs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "manEehMeimHD"
      },
      "source": [
        "#### Download and load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "g2cvJYNyjlOy",
        "outputId": "fd87608b-35d0-4532-b855-1cb8754c6587"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  721k  100  721k    0     0  4626k      0 --:--:-- --:--:-- --:--:-- 4746k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100 94400  100 94400    0     0  1097k      0 --:--:-- --:--:-- --:--:-- 1298k\n"
          ]
        }
      ],
      "source": [
        "!curl -o train-sent.txt https://raw.githubusercontent.com/Tsegaye-misikir/NLP-rug/main/sentiment/train-sent.txt\n",
        "!curl -o dev-sent.txt https://raw.githubusercontent.com/Tsegaye-misikir/NLP-rug/main/sentiment/dev-sent.txt\n",
        "train_file = 'train-sent.txt'\n",
        "dev_file = 'dev-sent.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulsaBn24p-LX"
      },
      "source": [
        "#### Indexer for examples\n",
        "This section contains code for an Indexer which is useful for creating a mapping between words and indices. It has already been implemented for you. Do read it and try to understand what the functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "Ck1X_FzPqMVk"
      },
      "outputs": [],
      "source": [
        "# Bijection between objects and integers starting at 0. Useful for mapping\n",
        "# labels, features, etc. into coordinates of a vector space.\n",
        "\n",
        "# This class creates a mapping between objects (here words) and unique indices\n",
        "# For example: apple->1, banana->2, and so on\n",
        "class Indexer(object):\n",
        "    def __init__(self):\n",
        "        self.objs_to_ints = {}\n",
        "        self.ints_to_objs = {}\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str([str(self.get_object(i)) for i in range(0, len(self))])\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.objs_to_ints)\n",
        "\n",
        "    # Returns the object corresponding to the particular index\n",
        "    def get_object(self, index):\n",
        "        if (index not in self.ints_to_objs):\n",
        "            return None\n",
        "        else:\n",
        "            return self.ints_to_objs[index]\n",
        "\n",
        "    def contains(self, object):\n",
        "        return self.index_of(object) != -1\n",
        "\n",
        "    # Returns -1 if the object isn't present, index otherwise\n",
        "    def index_of(self, object):\n",
        "        if (object not in self.objs_to_ints):\n",
        "            return -1\n",
        "        else:\n",
        "            return self.objs_to_ints[object]\n",
        "\n",
        "    # Adds the object to the index if it isn't present, always returns a nonnegative index\n",
        "    def add_and_get_index(self, object, add=True):\n",
        "        if not add:\n",
        "            return self.index_of(object)\n",
        "        if (object not in self.objs_to_ints):\n",
        "            new_idx = len(self.objs_to_ints)\n",
        "            self.objs_to_ints[object] = new_idx\n",
        "            self.ints_to_objs[new_idx] = object\n",
        "        return self.objs_to_ints[object]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIN8X-vpkeBh",
        "outputId": "aaada35c-8c85-4a78-bdeb-0aeb4a0da78a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train:  [['a', 'stirring', ',', 'funny', 'and', 'finally', 'transporting', 're-imagining', 'of', 'beauty', 'and', 'the', 'beast', 'and', '1930s', 'horror', 'films']; label=1, ['apparently', 'reassembled', 'from', 'the', 'cutting-room', 'floor', 'of', 'any', 'given', 'daytime', 'soap', '.']; label=0, ['they', 'presume', 'their', 'audience', 'wo', \"n't\", 'sit', 'still', 'for', 'a', 'sociology', 'lesson', ',', 'however', 'entertainingly', 'presented', ',', 'so', 'they', 'trot', 'out', 'the', 'conventional', 'science-fiction', 'elements', 'of', 'bug-eyed', 'monsters', 'and', 'futuristic', 'women', 'in', 'skimpy', 'clothes', '.']; label=0, ['this', 'is', 'a', 'visually', 'stunning', 'rumination', 'on', 'love', ',', 'memory', ',', 'history', 'and', 'the', 'war', 'between', 'art', 'and', 'commerce', '.']; label=1, ['jonathan', 'parker', \"'s\", 'bartleby', 'should', 'have', 'been', 'the', 'be-all-end-all', 'of', 'the', 'modern-office', 'anomie', 'films', '.']; label=1]\n",
            "dev:  [['one', 'long', 'string', 'of', 'cliches', '.']; label=0, ['if', 'you', \"'ve\", 'ever', 'entertained', 'the', 'notion', 'of', 'doing', 'what', 'the', 'title', 'of', 'this', 'film', 'implies', ',', 'what', 'sex', 'with', 'strangers', 'actually', 'shows', 'may', 'put', 'you', 'off', 'the', 'idea', 'forever', '.']; label=0, ['k-19', 'exploits', 'our', 'substantial', 'collective', 'fear', 'of', 'nuclear', 'holocaust', 'to', 'generate', 'cheap', 'hollywood', 'tension', '.']; label=0, ['it', \"'s\", 'played', 'in', 'the', 'most', 'straight-faced', 'fashion', ',', 'with', 'little', 'humor', 'to', 'lighten', 'things', 'up', '.']; label=0, ['there', 'is', 'a', 'fabric', 'of', 'complex', 'ideas', 'here', ',', 'and', 'feelings', 'that', 'profoundly', 'deepen', 'them', '.']; label=1]\n",
            "6920 train examples: 3610 positive, 3310 negative\n",
            "872 dev examples\n"
          ]
        }
      ],
      "source": [
        "# Load the data from the files\n",
        "train_exs = read_sentiment_examples(train_file)\n",
        "print(\"train: \", train_exs[:5])\n",
        "dev_exs = read_sentiment_examples(dev_file)\n",
        "print(\"dev: \", dev_exs[:5])\n",
        "n_pos = 0\n",
        "n_neg = 0\n",
        "for ex in train_exs:\n",
        "    if ex.label == 1:\n",
        "        n_pos += 1\n",
        "    else:\n",
        "        n_neg += 1\n",
        "print(\"%d train examples: %d positive, %d negative\" % (len(train_exs), n_pos, n_neg))\n",
        "print(\"%d dev examples\" % len(dev_exs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaC07CdvlB-9"
      },
      "source": [
        "### Define Logistic Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "u0b4C1DImvuM"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from collections import Counter\n",
        "from typing import List\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSCzfcnMjm-r"
      },
      "source": [
        "#### Define feature extractors\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "_eGn0zEOnLOq"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Union\n",
        "# Feature extraction base type. Takes an example and returns an indexed list of features.\n",
        "class FeatureExtractor(object):\n",
        "    # Extract features. Includes a flag add_to_indexer to control whether the indexer should be expanded.\n",
        "    # At test time, any unseen features should be discarded, but at train time, we probably want to keep growing it.\n",
        "    def extract_features(self, ex: \"SentimentExample\", add_to_indexer: bool) -> Dict[int, float]:\n",
        "        raise Exception(\"Don't call me, call my subclasses\")\n",
        "\n",
        "\n",
        "# Extracts unigram bag-of-words features from a sentence. It's up to you to decide how you want to handle counts\n",
        "class UnigramFeatureExtractor(FeatureExtractor):\n",
        "    def __init__(self, indexer: Indexer):\n",
        "        self.indexer = indexer\n",
        "\n",
        "    def extract_features(self, ex:\"SentimentExample\", add_to_indexer=False) -> Dict[int, float]:\n",
        "        features: Dict[int, float] = Counter()\n",
        "        for w in ex.words:\n",
        "            feat_idx: int = self.indexer.add_and_get_index(w) \\\n",
        "                if add_to_indexer else self.indexer.index_of(w)\n",
        "            if feat_idx != -1:\n",
        "                features[feat_idx] += 1.0\n",
        "        return features\n",
        "\n",
        "\n",
        "# Bigram feature extractor analogous to the unigram one.\n",
        "class BigramFeatureExtractor(FeatureExtractor):\n",
        "    def __init__(self, indexer: Indexer):\n",
        "        self.indexer = indexer\n",
        "\n",
        "    def extract_features(self, ex:\"SentimentExample\", add_to_indexer=False) -> Dict[int, float]:\n",
        "        features: Dict[int, float] = Counter()\n",
        "        for i in range(len(ex.words) - 1):\n",
        "            w = ex.words[i] + \"||\" + ex.words[i + 1]\n",
        "            feat_idx = self.indexer.add_and_get_index(w) \\\n",
        "                if add_to_indexer else self.indexer.index_of(w)\n",
        "            if feat_idx != -1:\n",
        "                features[feat_idx] += 1.0\n",
        "        return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLEP8hSXnvP5"
      },
      "source": [
        "#### Define base classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "7CgFsVR4nyGN"
      },
      "outputs": [],
      "source": [
        "# Sentiment classifier base type\n",
        "class SentimentClassifier(object):\n",
        "    # Makes a prediction for the given\n",
        "    def predict(self, ex: \"SentimentExample\"):\n",
        "        raise Exception(\"Don't call me, call my subclasses\")\n",
        "\n",
        "\n",
        "# Always predicts the positive class\n",
        "class AlwaysPositiveClassifier(SentimentClassifier):\n",
        "    def predict(self, ex: \"SentimentExample\") -> int:\n",
        "        return 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKEB5sYeodoh"
      },
      "source": [
        "#### Logistic Regression class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kcp4Wkbqoiuu"
      },
      "outputs": [],
      "source": [
        "class LogisticRegressionClassifier(SentimentClassifier):\n",
        "    def __init__(self, feat_extractor: FeatureExtractor, train_examples, num_iters=50, reg_lambda=0.0, learning_rate=0.1):\n",
        "        # TODO: Initialize the logistic regression model\n",
        "\n",
        "        # Arguments: feat_extractor is unigram or bigram, train_examples is train dataset\n",
        "        # num_iters is the number of epochs, reg_lambda is the regularization parameter\n",
        "        # learning_rate is the learning rate used in gradient descent\n",
        "\n",
        "        # STEP 1: Define variables for weights and biases, and initialize them\n",
        "\n",
        "        # STEP 2: Call the train() function. (This has already been done for you)\n",
        "\n",
        "        ##### SOLUTION START #####\n",
        "        shape = len(feat_extractor.indexer)\n",
        "        self.feature_ext = feat_extractor\n",
        "        self.weights = np.random.uniform(0, reg_lambda, shape)\n",
        "        self.biases = 0\n",
        "\n",
        "        ##### SOLUTION END #####\n",
        "\n",
        "        self.train(feat_extractor, train_examples, num_iters, reg_lambda, learning_rate)\n",
        "\n",
        "    def train(self, feat_extractor: FeatureExtractor, train_examples:list[\"SentimentExample\"], num_iters=50, reg_lambda=0.0, learning_rate=0.1):\n",
        "        # TODO: Function for training the logistic regression model\n",
        "\n",
        "        # STEP 1: Write a 'for' loop which iterates over the dataset num_iters times\n",
        "\n",
        "        # STEP 2: Write an inner 'for' loop for each step of gradient descent\n",
        "        # You can use stochastic gradient descent or mini-batch SGD\n",
        "\n",
        "        # STEP 3: In each step of gradient descent apply the update rule\n",
        "        # to weights and biases\n",
        "\n",
        "        ##### SOLUTION START #####\n",
        "\n",
        "        for epoch in range(num_iters): # num_iters is the number of epochs\n",
        "            for ex in train_examples: # iterate over the dataset\n",
        "                x = feat_extractor.extract_features(ex, add_to_indexer=True)\n",
        "                y = ex.label\n",
        "\n",
        "                # Compute linear combination of weights and features\n",
        "                # z = np.dot(self.weights, np.array(list(x.values()))) + self.biases # copilot\n",
        "                z = np.dot(self.weights, x) + self.biases # TODO: x might have to be converted into a np.array\n",
        "                y_hat = self.sigmoid(z)\n",
        "\n",
        "                # Compute gradients\n",
        "                gradient_w = (y_hat - y) * x + reg_lambda * self.weights # TODO: x: np.array?n\n",
        "                gradient_b = (y_hat - y)\n",
        "\n",
        "                # Update weights and biases\n",
        "                self.weights -= learning_rate * gradient_w\n",
        "                self.biases -= learning_rate * gradient_b\n",
        "\n",
        "        ##### SOLUTION END #####\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Sigmoid function for logistic regression.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z)) \n",
        "\n",
        "    def predict(self, ex):\n",
        "        # TODO: Logistic regression model's prediction for a single example\n",
        "\n",
        "        ##### SOLUTION START #####\n",
        "\n",
        "        x = self.feat_extractor.extract_features(ex, add_to_indexer=False)\n",
        "        z = np.dot(self.weights, x) + self.biases\n",
        "        return 1 if self.sigmoid(z) > 0.5 else 0\n",
        "\n",
        "        ##### SOLUTION END #####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cesGHOCipwgL"
      },
      "source": [
        "#### Training function for logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLwZpsU3qC31"
      },
      "outputs": [],
      "source": [
        "# Train a logsitic regression model on the given training examples using the given FeatureExtractor\n",
        "def train_lr(train_exs: List[\"SentimentExample\"], feat_extractor: FeatureExtractor, reg_lambda) -> LogisticRegressionClassifier:\n",
        "    # TODO: Function for training logistic regression model.\n",
        "    # Populate the feature_extractor.\n",
        "    # Initialize and return an object of instance LogisticRegressionClassifier\n",
        "\n",
        "    ##### SOLUTION START #####\n",
        "    X_train: list[\"SentimentExample\"] = [feat_extractor.extract_features(ex) for ex in train_exs]  \n",
        "    \n",
        "    # Convert to numpy arrays\n",
        "    X_train = np.array(X_train)\n",
        "    #y_train = np.array(y_train)\n",
        "    \n",
        "    # Initialize and train the logistic regression model\n",
        "    model = LogisticRegressionClassifier(feat_extractor, X_train, reg_lambda=reg_lambda)\n",
        "    model.train(feat_extractor, )\n",
        "    \n",
        "    return model\n",
        "    ##### SOLUTION END #####\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "xMHrP9X7qItd"
      },
      "outputs": [],
      "source": [
        "# Main entry point for your modifications. Trains and returns one of several models depending on the options passed\n",
        "def train_model(feature_type:str, model_type:str, train_exs, reg_lambda=0.0):\n",
        "    # Initialize feature extractor\n",
        "    if feature_type == \"unigram\":\n",
        "        # Add additional preprocessing code here\n",
        "        feat_extractor = UnigramFeatureExtractor(Indexer())\n",
        "    elif feature_type == \"bigram\":\n",
        "        # Add additional preprocessing code here\n",
        "        feat_extractor = BigramFeatureExtractor(Indexer())\n",
        "    else:\n",
        "        raise Exception(\"Pass unigram or bigram\")\n",
        "\n",
        "    # Train the model\n",
        "    if model_type == \"AlwaysPositive\":\n",
        "        model = AlwaysPositiveClassifier()\n",
        "    elif model_type == \"LogisticRegression\":\n",
        "        model = train_lr(train_exs, feat_extractor, reg_lambda=reg_lambda)\n",
        "    else:\n",
        "        raise Exception(\"Pass AlwaysPositive or LogisticRegression\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO3EqbavuJpz"
      },
      "source": [
        "### Functions for evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "mgTqmbGcuM3f"
      },
      "outputs": [],
      "source": [
        "# Evaluates a given classifier on the given examples\n",
        "def evaluate(classifier, exs):\n",
        "    return print_evaluation([ex.label for ex in exs], [classifier.predict(ex) for ex in exs])\n",
        "\n",
        "\n",
        "# Prints accuracy comparing golds and predictions, each of which is a sequence of 0/1 labels.\n",
        "def print_evaluation(golds, predictions):\n",
        "    num_correct = 0\n",
        "    num_pos_correct = 0\n",
        "    num_pred = 0\n",
        "    num_gold = 0\n",
        "    num_total = 0\n",
        "    if len(golds) != len(predictions):\n",
        "        raise Exception(\"Mismatched gold/pred lengths: %i / %i\" %\n",
        "                        (len(golds), len(predictions)))\n",
        "    for idx in range(0, len(golds)):\n",
        "        gold = golds[idx]\n",
        "        prediction = predictions[idx]\n",
        "        if prediction == gold:\n",
        "            num_correct += 1\n",
        "        if prediction == 1:\n",
        "            num_pred += 1\n",
        "        if gold == 1:\n",
        "            num_gold += 1\n",
        "        if prediction == 1 and gold == 1:\n",
        "            num_pos_correct += 1\n",
        "        num_total += 1\n",
        "\n",
        "    print(\"Accuracy: %i / %i = %.2f %%\" %\n",
        "          (num_correct, num_total,\n",
        "           num_correct * 100.0 / num_total))\n",
        "    return num_correct * 100.0 / num_total\n",
        "\n",
        "# Evaluate on train and dev dataset\n",
        "def eval_train_dev(model):\n",
        "    print(\"===== Train Accuracy =====\")\n",
        "    train_acc = evaluate(model, train_exs)\n",
        "    print(\"===== Dev Accuracy =====\")\n",
        "    eval_acc = evaluate(model, dev_exs)\n",
        "    return [train_acc, eval_acc]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwrQ1_lHv-Co"
      },
      "source": [
        "### Unigram vs Bigram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "Z1KNbz32wwws"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "LogisticRegressionClassifier.__init__() missing 2 required positional arguments: 'feat_extractor' and 'train_examples'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[86], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate logistic regression with unigram features\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m unigram_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munigram\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLogisticRegression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_exs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m eval_train_dev(unigram_model)\n",
            "Cell \u001b[1;32mIn[84], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(feature_type, model_type, train_exs, reg_lambda)\u001b[0m\n\u001b[0;32m     15\u001b[0m     model \u001b[38;5;241m=\u001b[39m AlwaysPositiveClassifier()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogisticRegression\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 17\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_exs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_extractor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreg_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg_lambda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPass AlwaysPositive or LogisticRegression\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[83], line 16\u001b[0m, in \u001b[0;36mtrain_lr\u001b[1;34m(train_exs, feat_extractor, reg_lambda)\u001b[0m\n\u001b[0;32m     13\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_train)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Initialize and train the logistic regression model\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegressionClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreg_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg_lambda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
            "\u001b[1;31mTypeError\u001b[0m: LogisticRegressionClassifier.__init__() missing 2 required positional arguments: 'feat_extractor' and 'train_examples'"
          ]
        }
      ],
      "source": [
        "# Evaluate logistic regression with unigram features\n",
        "unigram_model = train_model('unigram', 'LogisticRegression', train_exs)\n",
        "eval_train_dev(unigram_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wF2xxlx2roLC"
      },
      "outputs": [],
      "source": [
        "# Evaluate logistic regression with bigram features\n",
        "unigram_model = train_model('bigram', 'LogisticRegression', train_exs)\n",
        "eval_train_dev(unigram_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jnCD2esA5Aa"
      },
      "source": [
        "### Logistic regression with regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgSNsC5kA9Gs"
      },
      "outputs": [],
      "source": [
        "# TODO: Experiment with different regularization parameters and plot train and dev accuracies\n",
        "# You can either hard code the values or,\n",
        "# write a loop that calculates accuracies for different parameters\n",
        "\n",
        "### SOLUTION START ###\n",
        "\n",
        "### SOLUTION END ###"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
